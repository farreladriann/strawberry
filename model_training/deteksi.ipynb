{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset_labels_with_yaml(base_path, yaml_file='data.yaml'):\n",
    "    \"\"\"\n",
    "    Menganalisis dataset penyakit dengan membaca file label dan menghitung distribusi kelas\n",
    "    berdasarkan angka pertama dari tiap baris pada file label, serta menggunakan file YAML\n",
    "    untuk memetakan ID kelas ke nama kelas.\n",
    "\n",
    "    Parameter:\n",
    "    - base_path: Path utama yang berisi folder train, test, dan valid.\n",
    "    - yaml_file: Path ke file data.yaml yang berisi mapping kelas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Baca file YAML untuk mendapatkan mapping ID kelas ke nama kelas\n",
    "    with open(os.path.join(base_path, yaml_file), 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "\n",
    "    class_names = yaml_data.get('names', [])\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Subfolder yang akan diproses\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "    # Inisialisasi dictionary untuk menyimpan jumlah data berdasarkan kelas\n",
    "    class_counts = {folder: {} for folder in folders}\n",
    "\n",
    "    # Proses setiap folder\n",
    "    for folder in folders:\n",
    "        labels_folder_path = os.path.join(base_path, folder, \"labels\")\n",
    "        if os.path.exists(labels_folder_path):\n",
    "            for label_file in os.listdir(labels_folder_path):\n",
    "                label_file_path = os.path.join(labels_folder_path, label_file)\n",
    "                with open(label_file_path, 'r') as file:\n",
    "                    for line in file:\n",
    "                        # Ambil angka pertama dari setiap baris (kelas)\n",
    "                        class_id = line.split()[0]\n",
    "                        if class_id not in class_counts[folder]:\n",
    "                            class_counts[folder][class_id] = 0\n",
    "                        class_counts[folder][class_id] += 1\n",
    "\n",
    "    # Output total data untuk setiap kelas dalam bentuk kolom\n",
    "    print(\"Total Data untuk Setiap Kelas:\")\n",
    "    print(\"{:<25} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Nama Kelas\", \"Train\", \"Test\", \"Valid\", \"Total\"\n",
    "    ))\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    total_semua = 0\n",
    "    for class_id in range(num_classes):\n",
    "        class_name = class_names[class_id]\n",
    "        train_count = class_counts[\"train\"].get(str(class_id), 0)\n",
    "        test_count = class_counts[\"test\"].get(str(class_id), 0)\n",
    "        valid_count = class_counts[\"valid\"].get(str(class_id), 0)\n",
    "        total_count = train_count + test_count + valid_count\n",
    "        total_semua += total_count\n",
    "        print(\"{:<25} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "            class_name, train_count, test_count, valid_count, total_count\n",
    "        ))\n",
    "    print(\"-\" * 65)\n",
    "    print(\"Total Keseluruhan Data: {}\".format(total_semua))\n",
    "\n",
    "    # Hitung persentase distribusi data untuk setiap kelas\n",
    "    print(\"\\nPersentase Distribusi Data untuk Setiap Kelas:\")\n",
    "    print(\"{:<25} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Nama Kelas\", \"Train (%)\", \"Test (%)\", \"Valid (%)\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    for class_id in range(num_classes):\n",
    "        class_name = class_names[class_id]\n",
    "        train_count = class_counts[\"train\"].get(str(class_id), 0)\n",
    "        test_count = class_counts[\"test\"].get(str(class_id), 0)\n",
    "        valid_count = class_counts[\"valid\"].get(str(class_id), 0)\n",
    "        total_count = train_count + test_count + valid_count\n",
    "        if total_count > 0:\n",
    "            train_percent = (train_count / total_count) * 100\n",
    "            test_percent = (test_count / total_count) * 100\n",
    "            valid_percent = (valid_count / total_count) * 100\n",
    "        else:\n",
    "            train_percent = test_percent = valid_percent = 0\n",
    "        print(\"{:<25} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "            class_name, train_percent, test_percent, valid_percent\n",
    "        ))\n",
    "\n",
    "    # Konversi ke DataFrame\n",
    "    df = pd.DataFrame(class_counts).fillna(0).astype(int)\n",
    "    df.index = [class_names[int(idx)] for idx in df.index]  # Ganti index dengan nama kelas\n",
    "\n",
    "    # Plot grafik\n",
    "    df.plot(kind='bar', figsize=(14, 7))\n",
    "    plt.title('Jumlah Data untuk Setiap Kelas di Setiap Folder')\n",
    "    plt.xlabel('Kelas')\n",
    "    plt.ylabel('Jumlah Data')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Folder', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_labels_and_images(base_path):\n",
    "    \"\"\"\n",
    "    Memeriksa apakah setiap file gambar memiliki file label yang sesuai, dan sebaliknya,\n",
    "    di dalam folder train, test, dan valid.\n",
    "\n",
    "    Parameter:\n",
    "    - base_path: Path utama yang berisi folder train, test, dan valid.\n",
    "    \"\"\"\n",
    "    # Subfolder yang akan diproses\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "    for folder in folders:\n",
    "        images_folder = os.path.join(base_path, folder, \"images\")\n",
    "        labels_folder = os.path.join(base_path, folder, \"labels\")\n",
    "\n",
    "        # Dapatkan daftar file di folder images dan labels\n",
    "        image_files = set(os.listdir(images_folder))\n",
    "        label_files = set(os.listdir(labels_folder))\n",
    "\n",
    "        # Loop melalui setiap file gambar untuk memeriksa label\n",
    "        missing_labels = []\n",
    "        for image_file in image_files:\n",
    "            # Ubah ekstensi file gambar menjadi .txt\n",
    "            label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
    "            if label_file not in label_files:\n",
    "                missing_labels.append(image_file)\n",
    "\n",
    "        # Loop melalui setiap file label untuk memeriksa gambar\n",
    "        missing_images = []\n",
    "        for label_file in label_files:\n",
    "            # Ubah ekstensi file label menjadi .jpg (atau ekstensi gambar lainnya)\n",
    "            image_file = os.path.splitext(label_file)[0] + \".jpg\"  # Sesuaikan dengan ekstensi gambar Anda\n",
    "            if image_file not in image_files:\n",
    "                missing_images.append(label_file)\n",
    "\n",
    "        # Tampilkan hasil\n",
    "        print(f\"Folder {folder}:\")\n",
    "        if missing_labels:\n",
    "            print(f\"  - {len(missing_labels)} file gambar tidak memiliki file label:\")\n",
    "            for file in missing_labels:\n",
    "                print(f\"    - {file}\")\n",
    "        else:\n",
    "            print(\"  - Semua file gambar memiliki file label.\")\n",
    "\n",
    "        if missing_images:\n",
    "            print(f\"  - {len(missing_images)} file label tidak memiliki file gambar:\")\n",
    "            for file in missing_images:\n",
    "                print(f\"    - {file}\")\n",
    "        else:\n",
    "            print(\"  - Semua file label memiliki file gambar.\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import filecmp\n",
    "\n",
    "def extract_base_name(file_name):\n",
    "    \"\"\"\n",
    "    Mengekstrak nama dasar file tanpa tambahan _jpg dan rf..\n",
    "    Contoh:\n",
    "    - leaf_spot214.rf.8bf5924ff9b331a07aa870740e6ad39d.jpg → leaf_spot214\n",
    "    - leaf_spot214_jpg.rf.0ab37a08b63b9917d4204c9f920d9c43.jpg → leaf_spot214\n",
    "    \"\"\"\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    if \"_jpg\" in file_name:\n",
    "        file_name = file_name.replace(\"_jpg\", \"\")\n",
    "    if \".rf\" in file_name:\n",
    "        file_name = file_name.split(\".rf\")[0]\n",
    "    return file_name\n",
    "\n",
    "def remove_duplicate_files_labels(base_path):\n",
    "    \"\"\"\n",
    "    Menghapus file duplikat berdasarkan nama dasar file dan konten label yang sama\n",
    "    di dalam folder train, test, dan valid.  Hanya menghapus jika labelnya sama persis.\n",
    "    Menampilkan peringatan jika nama dasar file image sama, walaupun labelnya beda.\n",
    "    \"\"\"\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "    base_name_to_files = defaultdict(list)\n",
    "\n",
    "    for folder in folders:\n",
    "        images_folder_path = os.path.join(base_path, folder, \"images\")\n",
    "        labels_folder_path = os.path.join(base_path, folder, \"labels\")  # Path ke folder label\n",
    "        if os.path.exists(images_folder_path) and os.path.exists(labels_folder_path): #Pastikan kedua folder ada\n",
    "            for file_name in os.listdir(images_folder_path):\n",
    "                if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    base_name = extract_base_name(file_name)\n",
    "                    label_file_name = file_name.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\").replace(\".jpeg\", \".txt\").replace(\".bmp\", \".txt\").replace(\".tiff\", \".txt\")\n",
    "                    label_path = os.path.join(labels_folder_path, label_file_name)\n",
    "                    if os.path.exists(label_path): # Pastikan file labelnya ada\n",
    "                        base_name_to_files[base_name].append((folder, file_name, label_path)) #Tambahkan label path\n",
    "                    else:\n",
    "                        print(f\"Warning: File label tidak ditemukan untuk {folder}/images/{file_name}\")\n",
    "        else:\n",
    "            print(f\"Warning: Folder images atau labels tidak ditemukan di {os.path.join(base_path, folder)}\")\n",
    "\n",
    "\n",
    "    for base_name, files in base_name_to_files.items():\n",
    "        if len(files) > 1:\n",
    "            files_to_delete = []\n",
    "            for i in range(len(files)):\n",
    "                for j in range(i + 1, len(files)):\n",
    "                    folder1, file1, label_path1 = files[i]\n",
    "                    folder2, file2, label_path2 = files[j]\n",
    "\n",
    "                    if filecmp.cmp(label_path1, label_path2, shallow=False): #Bandingkan isinya\n",
    "                        print(f\"File berikut memiliki nama dasar dan label yang sama ({base_name}):\")\n",
    "                        print(f\"- {folder1}/images/{file1}\")\n",
    "                        print(f\"- {folder2}/images/{file2}\")\n",
    "\n",
    "                        # Pilih file pertama untuk disimpan, hapus file kedua (konsisten)\n",
    "                        files_to_delete.append((folder2, file2, label_path2))  # Tambahkan ke daftar untuk dihapus\n",
    "                        print(\"-\" * 40)\n",
    "                    else:\n",
    "                        print(f\"Warning: File dengan nama dasar yang sama ({base_name}) memiliki label yang berbeda:\")\n",
    "                        print(f\"- {folder1}/images/{file1}\")\n",
    "                        print(f\"- {folder2}/images/{file2}\")\n",
    "                        print(\"-\" * 40)\n",
    "\n",
    "            for folder_to_delete, file_to_delete, label_path_to_delete in files_to_delete:\n",
    "                image_path = os.path.join(base_path, folder_to_delete, \"images\", file_to_delete)\n",
    "                label_path = label_path_to_delete\n",
    "                if os.path.exists(image_path):\n",
    "                    os.remove(image_path)\n",
    "                    print(f\"- Dihapus: {folder_to_delete}/images/{file_to_delete}\")\n",
    "                else:\n",
    "                    print(f\"- File gambar tidak ditemukan: {folder_to_delete}/images/{file_to_delete}\")\n",
    "                if os.path.exists(label_path):\n",
    "                    os.remove(label_path)\n",
    "                    print(f\"- Dihapus: {folder_to_delete}/labels/{os.path.basename(label_path)}\")\n",
    "                else:\n",
    "                    print(f\"- File label tidak ditemukan: {folder_to_delete}/labels/{os.path.basename(label_path)}\")\n",
    "                print(\"-\" * 40)\n",
    "\n",
    "    print(\"Proses penghapusan duplikat selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from typing import Dict\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    return name.lower().replace('-', ' ').replace('_', ' ')\n",
    "\n",
    "def combined_data_yaml(source_folder: str, target_folder: str) -> None:\n",
    "    source_yaml = os.path.join(source_folder, \"data.yaml\")\n",
    "    target_yaml = os.path.join(target_folder, \"data.yaml\")\n",
    "\n",
    "    with open(source_yaml, \"r\") as file:\n",
    "        source_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    with open(target_yaml, \"r\") as file:\n",
    "        target_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    normalized_combined = [normalize_name(name) for name in target_data[\"names\"]]\n",
    "\n",
    "    for name in source_data[\"names\"]:\n",
    "        normalized_name = normalize_name(name)\n",
    "        if normalized_name not in normalized_combined:\n",
    "            normalized_combined.append(normalized_name)\n",
    "\n",
    "    combined_data = {\n",
    "        \"train\": target_data[\"train\"],\n",
    "        \"val\": target_data[\"val\"],\n",
    "        \"test\": target_data[\"test\"],\n",
    "        \"names\": normalized_combined,\n",
    "        \"nc\": len(normalized_combined)\n",
    "    }\n",
    "\n",
    "    with open(target_yaml, \"w\") as file:\n",
    "        yaml.dump(combined_data, file)\n",
    "\n",
    "# return dictionary of names\n",
    "def dictionary_yaml_old_to_new(source_folder: str, target_folder_with_new_data_yaml: str) -> Dict[int, int]:\n",
    "    source_yaml = os.path.join(source_folder, \"data.yaml\")\n",
    "    target_yaml = os.path.join(target_folder_with_new_data_yaml, \"data.yaml\")\n",
    "\n",
    "    with open(source_yaml, \"r\") as file:\n",
    "        source_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    with open(target_yaml, \"r\") as file:\n",
    "        target_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    source_names = source_data[\"names\"]\n",
    "    target_names = target_data[\"names\"]\n",
    "\n",
    "    dictionary = {}\n",
    "    for i, name in enumerate(source_names):\n",
    "        normalized_name = normalize_name(name)\n",
    "        if normalized_name in target_names:\n",
    "            dictionary[i] = target_names.index(normalized_name)\n",
    "        else:\n",
    "            # ERROR\n",
    "            print(f\"{name} not found in target data.yaml\")\n",
    "            return {}\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def move_file(source_file: str, target_folder: str) -> None:\n",
    "    source_file_name = os.path.basename(source_file)\n",
    "    target_file = os.path.join(target_folder, source_file_name)\n",
    "    if os.path.exists(target_file):\n",
    "        # ERROR\n",
    "        raise FileExistsError(\"File already exists in target folder\")\n",
    "    else:\n",
    "        os.rename(source_file, target_file)\n",
    "\n",
    "def move_label(source_label: str, target_folder: str, mapping: Dict[int, int]) -> None:\n",
    "    source_label_name = os.path.basename(source_label)\n",
    "    target_label = os.path.join(target_folder, source_label_name)\n",
    "    if os.path.exists(target_label):\n",
    "        # ERROR\n",
    "        raise FileExistsError(\"File already exists in target folder\")\n",
    "    else:\n",
    "        with open(source_label, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        with open(target_label, \"w\") as file:\n",
    "            for line in lines:\n",
    "                line = line.split()\n",
    "                line[0] = str(mapping[int(line[0])])\n",
    "                line = \" \".join(line)\n",
    "                file.write(line + \"\\n\")\n",
    "\n",
    "def combine_dataset(source_folder: str, target_folder: str) -> None:\n",
    "    main_folders = [\"train\", \"valid\", \"test\"]\n",
    "    for main_folder in main_folders:\n",
    "        # move images and labels\n",
    "        source_images_folder = os.path.join(source_folder, main_folder, \"images\")\n",
    "        target_images_folder = os.path.join(target_folder, main_folder, \"images\")\n",
    "\n",
    "        source_labels_folder = os.path.join(source_folder, main_folder, \"labels\")\n",
    "        target_labels_folder = os.path.join(target_folder, main_folder, \"labels\")\n",
    "\n",
    "        combined_data_yaml(source_folder, target_folder)\n",
    "        mapping = dictionary_yaml_old_to_new(source_folder, target_folder)\n",
    "        if mapping == {}:  # ERROR\n",
    "            return\n",
    "\n",
    "        for source_image in os.listdir(source_images_folder):\n",
    "            source_image_path = os.path.join(source_images_folder, source_image)\n",
    "            move_file(source_image_path, target_images_folder)\n",
    "\n",
    "            source_label = source_image.replace(\".jpg\", \".txt\")\n",
    "            source_label_path = os.path.join(source_labels_folder, source_label)\n",
    "            move_label(source_label_path, target_labels_folder, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from collections import defaultdict\n",
    "\n",
    "def remove_duplicate_images(base_path):\n",
    "    \"\"\"\n",
    "    Remove duplicate images and their corresponding labels from a dataset.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the dataset directory containing train, test, and valid folders\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Subfolder yang akan diproses\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "    def calculate_image_hash(image_path):\n",
    "        \"\"\"\n",
    "        Menghitung hash gambar menggunakan perceptual hashing (pHash).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            return imagehash.average_hash(img)  # Menggunakan pHash (average hash)\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal memproses {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_image_hashes_from_folder(folder_path):\n",
    "        \"\"\"\n",
    "        Membaca semua gambar dari folder dan mengembalikan dictionary\n",
    "        yang berisi nama file dan hash-nya.\n",
    "        \"\"\"\n",
    "        image_hashes = {}\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                img_hash = calculate_image_hash(file_path)\n",
    "                if img_hash is not None:\n",
    "                    image_hashes[file_name] = img_hash\n",
    "        return image_hashes\n",
    "\n",
    "    # Load semua hash gambar dari setiap folder\n",
    "    all_image_hashes = {}\n",
    "    for folder in folders:\n",
    "        images_folder_path = os.path.join(base_path, folder, \"images\")\n",
    "        if os.path.exists(images_folder_path):\n",
    "            all_image_hashes[folder] = load_image_hashes_from_folder(images_folder_path)\n",
    "\n",
    "    # Cek duplikat gambar berdasarkan hash\n",
    "    hash_to_files = defaultdict(list)\n",
    "\n",
    "    # Kelompokkan file berdasarkan hash\n",
    "    for folder in folders:\n",
    "        for file_name, img_hash in all_image_hashes.get(folder, {}).items():\n",
    "            hash_to_files[img_hash].append((folder, file_name))\n",
    "\n",
    "    # Proses dan hapus duplikat\n",
    "    for img_hash, files in hash_to_files.items():\n",
    "        if len(files) > 1:\n",
    "            print(\"Gambar berikut memiliki hash yang sama:\")\n",
    "            for folder, file_name in files:\n",
    "                print(f\"- {folder}/images/{file_name}\")\n",
    "\n",
    "            # Pertahankan file pertama, hapus sisanya\n",
    "            files_to_delete = files[1:]\n",
    "\n",
    "            for folder_to_delete, file_to_delete in files_to_delete:\n",
    "                # Path gambar\n",
    "                image_path = os.path.join(base_path, folder_to_delete, \"images\", file_to_delete)\n",
    "\n",
    "                # Path label\n",
    "                label_path = os.path.join(base_path, folder_to_delete, \"labels\",\n",
    "                                        os.path.splitext(file_to_delete)[0] + \".txt\")\n",
    "\n",
    "                # Hapus gambar\n",
    "                if os.path.exists(image_path):\n",
    "                    os.remove(image_path)\n",
    "                    print(f\"- Dihapus: {folder_to_delete}/images/{file_to_delete}\")\n",
    "\n",
    "                # Hapus label\n",
    "                if os.path.exists(label_path):\n",
    "                    os.remove(label_path)\n",
    "                    print(f\"- Dihapus: {folder_to_delete}/labels/{os.path.basename(label_path)}\")\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    print(\"Proses penghapusan duplikat selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "from collections import defaultdict\n",
    "\n",
    "def check_duplicate_images(base_path):\n",
    "    \"\"\"\n",
    "    Check and count duplicate images in a dataset without deleting them.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the dataset directory containing train, test, and valid folders\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (total_duplicates, dict of duplicate groups)\n",
    "    \"\"\"\n",
    "    # Subfolder yang akan diproses\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "    total_duplicates = 0\n",
    "    duplicate_groups = []\n",
    "\n",
    "    def calculate_image_hash(image_path):\n",
    "        \"\"\"\n",
    "        Menghitung hash gambar menggunakan perceptual hashing (pHash).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            return imagehash.average_hash(img)  # Menggunakan pHash (average hash)\n",
    "        except Exception as e:\n",
    "            print(f\"Gagal memproses {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_image_hashes_from_folder(folder_path):\n",
    "        \"\"\"\n",
    "        Membaca semua gambar dari folder dan mengembalikan dictionary\n",
    "        yang berisi nama file dan hash-nya.\n",
    "        \"\"\"\n",
    "        image_hashes = {}\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                img_hash = calculate_image_hash(file_path)\n",
    "                if img_hash is not None:\n",
    "                    image_hashes[file_name] = img_hash\n",
    "        return image_hashes\n",
    "\n",
    "    # Load semua hash gambar dari setiap folder\n",
    "    all_image_hashes = {}\n",
    "    for folder in folders:\n",
    "        images_folder_path = os.path.join(base_path, folder, \"images\")\n",
    "        if os.path.exists(images_folder_path):\n",
    "            all_image_hashes[folder] = load_image_hashes_from_folder(images_folder_path)\n",
    "\n",
    "    # Cek duplikat gambar berdasarkan hash\n",
    "    hash_to_files = defaultdict(list)\n",
    "\n",
    "    # Kelompokkan file berdasarkan hash\n",
    "    for folder in folders:\n",
    "        for file_name, img_hash in all_image_hashes.get(folder, {}).items():\n",
    "            hash_to_files[img_hash].append((folder, file_name))\n",
    "\n",
    "    # Proses dan hitung duplikat\n",
    "    for img_hash, files in hash_to_files.items():\n",
    "        if len(files) > 1:\n",
    "            print(\"\\nGrup gambar duplikat ditemukan:\")\n",
    "            for folder, file_name in files:\n",
    "                print(f\"- {folder}/images/{file_name}\")\n",
    "            \n",
    "            # Hitung jumlah file yang akan dihapus (semua file kecuali yang pertama)\n",
    "            num_duplicates = len(files) - 1\n",
    "            total_duplicates += num_duplicates\n",
    "            \n",
    "            # Simpan informasi grup duplikat\n",
    "            duplicate_groups.append({\n",
    "                'original': f\"{files[0][0]}/images/{files[0][1]}\",\n",
    "                'duplicates': [f\"{f[0]}/images/{f[1]}\" for f in files[1:]]\n",
    "            })\n",
    "            \n",
    "            print(f\"Jumlah duplikat dalam grup ini: {num_duplicates}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    print(f\"\\nRingkasan:\")\n",
    "    print(f\"Total gambar duplikat yang ditemukan: {total_duplicates}\")\n",
    "    print(f\"Total grup duplikat: {len([g for g in hash_to_files.values() if len(g) > 1])}\")\n",
    "    \n",
    "    return total_duplicates, duplicate_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_dataset_labels_for_classification(base_path, yaml_file='data.yaml'):\n",
    "    \"\"\"\n",
    "    Menganalisis dataset citra untuk tugas klasifikasi berdasarkan file label.\n",
    "    Setiap file label diasumsikan hanya memiliki satu baris (atau baris pertama yang relevan),\n",
    "    dan angka pertama pada baris tersebut merupakan ID kelas.\n",
    "\n",
    "    Parameter:\n",
    "    - base_path: Path utama yang berisi folder train, test, dan valid.\n",
    "    - yaml_file: Path ke file data.yaml yang berisi mapping kelas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Baca file YAML untuk mendapatkan mapping ID kelas ke nama kelas\n",
    "    with open(os.path.join(base_path, yaml_file), 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "\n",
    "    class_names = yaml_data.get('names', [])\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Folder-folder yang akan diproses\n",
    "    folders = [\"train\", \"test\", \"valid\"]\n",
    "\n",
    "    # Inisialisasi dictionary untuk menyimpan jumlah data berdasarkan kelas per folder\n",
    "    # Inisialisasi semua kelas dengan 0 untuk memastikan setiap kelas ada meskipun count-nya 0\n",
    "    class_counts = {folder: {str(i): 0 for i in range(num_classes)} for folder in folders}\n",
    "\n",
    "    # Proses setiap folder\n",
    "    for folder in folders:\n",
    "        labels_folder_path = os.path.join(base_path, folder, \"labels\")\n",
    "        if os.path.exists(labels_folder_path):\n",
    "            for label_file in os.listdir(labels_folder_path):\n",
    "                label_file_path = os.path.join(labels_folder_path, label_file)\n",
    "                with open(label_file_path, 'r') as file:\n",
    "                    # Baca baris pertama saja karena untuk klasifikasi kita hanya membutuhkan label utama\n",
    "                    line = file.readline().strip()\n",
    "                    if not line:\n",
    "                        continue  # Skip jika file kosong\n",
    "                    # Ambil angka pertama dari baris tersebut sebagai ID kelas\n",
    "                    class_id = line.split()[0]\n",
    "                    class_counts[folder][class_id] += 1\n",
    "\n",
    "    # Output total data untuk setiap kelas\n",
    "    print(\"Total Data untuk Setiap Kelas:\")\n",
    "    print(\"{:<25} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Nama Kelas\", \"Train\", \"Test\", \"Valid\", \"Total\"\n",
    "    ))\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    total_semua = 0\n",
    "    for i in range(num_classes):\n",
    "        class_name = class_names[i]\n",
    "        # Ambil jumlah data per folder dari dictionary yang sudah diinisialisasi\n",
    "        train_count = class_counts[\"train\"].get(str(i), 0)\n",
    "        test_count  = class_counts[\"test\"].get(str(i), 0)\n",
    "        valid_count = class_counts[\"valid\"].get(str(i), 0)\n",
    "        total_count = train_count + test_count + valid_count\n",
    "        total_semua += total_count\n",
    "        print(\"{:<25} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "            class_name, train_count, test_count, valid_count, total_count\n",
    "        ))\n",
    "    print(\"-\" * 65)\n",
    "    print(\"Total Keseluruhan Data: {}\".format(total_semua))\n",
    "\n",
    "    # Hitung dan tampilkan persentase distribusi data untuk setiap kelas\n",
    "    print(\"\\nPersentase Distribusi Data untuk Setiap Kelas:\")\n",
    "    print(\"{:<25} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Nama Kelas\", \"Train (%)\", \"Test (%)\", \"Valid (%)\"\n",
    "    ))\n",
    "    print(\"-\" * 55)\n",
    "    for i in range(num_classes):\n",
    "        class_name = class_names[i]\n",
    "        train_count = class_counts[\"train\"].get(str(i), 0)\n",
    "        test_count  = class_counts[\"test\"].get(str(i), 0)\n",
    "        valid_count = class_counts[\"valid\"].get(str(i), 0)\n",
    "        total_count = train_count + test_count + valid_count\n",
    "        if total_count > 0:\n",
    "            train_percent = (train_count / total_count) * 100\n",
    "            test_percent  = (test_count / total_count) * 100\n",
    "            valid_percent = (valid_count / total_count) * 100\n",
    "        else:\n",
    "            train_percent = test_percent = valid_percent = 0\n",
    "        print(\"{:<25} {:<10.2f} {:<10.2f} {:<10.2f}\".format(\n",
    "            class_name, train_percent, test_percent, valid_percent\n",
    "        ))\n",
    "\n",
    "    # Konversi data ke DataFrame untuk keperluan plotting\n",
    "    df = pd.DataFrame(class_counts).fillna(0).astype(int)\n",
    "\n",
    "    # Pastikan index DataFrame (ID kelas) diubah menjadi nama kelas yang sesuai\n",
    "    df.index = [class_names[int(idx)] for idx in df.index]\n",
    "\n",
    "    # Plot grafik batang\n",
    "    df.plot(kind='bar', figsize=(14, 7))\n",
    "    plt.title('Jumlah Data untuk Setiap Kelas di Setiap Folder')\n",
    "    plt.xlabel('Kelas')\n",
    "    plt.ylabel('Jumlah Data')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Folder', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in strawberry-2 to yolov11::   4%|▎         | 107885/2938620 [01:13<32:00, 1474.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m project \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mworkspace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfarrelganteng\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mproject(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrawberry-skcti-ew21i\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m version \u001b[38;5;241m=\u001b[39m project\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov11\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/roboflow/core/version.py:233\u001b[0m, in \u001b[0;36mVersion.download\u001b[0;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    231\u001b[0m             response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__download_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__extract_zip(location, model_format)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reformat_yaml(location, model_format)  \u001b[38;5;66;03m# TODO: is roboflow-python a place to be munging yaml files?\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/roboflow/core/version.py:839\u001b[0m, in \u001b[0;36mVersion.__download_zip\u001b[0;34m(self, link, location, format)\u001b[0m\n\u001b[1;32m    837\u001b[0m total_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    838\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m TQDM_DISABLE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Dataset Version Zip in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 839\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/urllib3/response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/usr/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/usr/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "def change_label_class(base_path: str, mappingNumber: Dict[int, int]) -> None:\n",
    "    for folder in [\"train\", \"valid\", \"test\"]:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        labels_folder_path = os.path.join(folder_path, \"labels\")\n",
    "        for file in os.listdir(labels_folder_path):\n",
    "            file_path = os.path.join(labels_folder_path, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "            with open(file_path, \"w\") as f:\n",
    "                for line in lines:\n",
    "                    class_number = int(line.split()[0])\n",
    "                    if class_number in mappingNumber:\n",
    "                        class_number = mappingNumber[class_number]\n",
    "                    f.write(f\"{class_number} {' '.join(line.split()[1:])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribusi_dataset(base_path: str, split_ratios: tuple):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
